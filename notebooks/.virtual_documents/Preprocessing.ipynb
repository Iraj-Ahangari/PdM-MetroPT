


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq
from datetime import datetime
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler, StandardScaler

print("Packages are available!")






!wget https://zenodo.org/records/6854240/files/dataset_train.csv?download=1 -O dataset_train.csv









#df = pd.read_csv('dataset_train.csv')
df = pd.read_csv('/mnt/e/PdM/DataX/MetroPT/dataset_train.csv')
df.head()


df.columns








df.drop(['gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'], axis=1, inplace=True)






# Print shape of the dataset
print('Shape of the dataset is: ', df.shape)

# Check for missing values in the DataFrame
missing_values = df.isnull().sum()

# Print the number of missing values in each column
print("Number of missing values in each column:")
print(missing_values)

# Get the data types of the columns
column_types = df.dtypes

# Print the data types of each column
print("\nData types of each column:")
print(column_types)





# Calculate the initial memory usage
initial_memory = df.memory_usage(deep=True).sum()
print(f'Initial memory usage: {initial_memory // 1e6} Mb')

# Convert integer columns to the smallest possible int type
for col in df.select_dtypes(include='int64').columns:
    df[col] = pd.to_numeric(df[col], downcast='integer')

# Convert float columns to float32
for col in df.select_dtypes(include='float64').columns:
    df[col] = df[col].astype('float32')

# Calculate the final memory usage after type conversion
final_memory = df.memory_usage(deep=True).sum()
print(f'Final memory usage: {final_memory // 1e6} Mb')

# Calculate and print the difference in memory usage
memory_difference = initial_memory - final_memory
print(f'Memory saved: {memory_difference // 1e3} byte')







# Add 'Failure Type' and 'Failure Component' columns with default values
df['Failure Type'] = 'No Failure'
df['Failure Component'] = 'No Failur Component'

# Define conditions and corresponding values
conditions_values = [
    ((df['timestamp'] >= '2022-02-28 21:53:00') &
     (df['timestamp'] < '2022-03-01 02:00:00'), 'Air Leak', 'Clients'),

    ((df['timestamp'] >= '2022-03-23 14:54:00') &
     (df['timestamp'] < '2022-03-23 15:24:00'), 'Air Leak', 'Air Dryer'),

    ((df['timestamp'] >= '2022-05-30 12:00:00') &
     (df['timestamp'] < '2022-06-02 06:18:00'), 'Oil Leak', 'Compressor')
]

# Loop through conditions and update values
for condition, failure_type, failure_component in conditions_values:
    df.loc[condition, 'Failure Type'] = failure_type
    df.loc[condition, 'Failure Component'] = failure_component






import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your pandas DataFrame with 'Failure Type' and 'Failure Component' as columns

# Calculate the counts
failure_type_counts = df['Failure Type'].value_counts()
failure_component_counts = df['Failure Component'].value_counts()

# Create subplots for bar charts
fig, axs = plt.subplots(1, 2, figsize=(15, 6))

# Bar chart for Failure Type
sns.barplot(x=failure_type_counts.index, y=failure_type_counts.values, ax=axs[0], palette="viridis")
axs[0].set_title('Distribution of Failure Types')
axs[0].set_ylabel('Count')
axs[0].tick_params(axis='x', rotation=45)

# Bar chart for Failure Component
sns.barplot(x=failure_component_counts.index, y=failure_component_counts.values, ax=axs[1], palette="magma")
axs[1].set_title('Distribution of Failure Components')
axs[1].set_ylabel('Count')
axs[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()






import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.dates as mdates

# Assume df is preloaded with the relevant data and 'timestamp' as the index in datetime format
# Ensure that 'timestamp' is a datetime type and set as the index
df['timestamp'] = pd.to_datetime(df['timestamp'])
df.set_index('timestamp', inplace=True)

# Define the time range for the x-axis based on your data
start_time = "2022-01-01 08:30:00"
end_time = "2022-01-01 10:30:00"

# Convert the time range to datetime format
start_time = pd.to_datetime(start_time)
end_time = pd.to_datetime(end_time)

# Filter the dataframe to only include the specified time range
df_time_filtered = df.loc[(df.index >= start_time) & (df.index <= end_time)]

# Define the list of sensor columns to plot (excluding non-sensor columns like 'Failure Type' and 'Failure Component')
sensor_columns = ['TP2', 'TP3', 'H1', 'DV_pressure', 'Reservoirs',
                  'Oil_temperature', 'Flowmeter', 'Motor_current']
# Define colors for the plot
colors = ['blue', 'red', 'cyan', 'purple', 'yellow', 'orange', 'pink', 'green']

# Create subplots
fig, axes = plt.subplots(nrows=len(sensor_columns), ncols=1, figsize=(15, 20), sharex=True)

# Set the background color for the entire figure
fig.patch.set_facecolor('#f5f5f5')  # Example: light grey background for the figure

# Plot each sensor data
for ax, sensor, color in zip(axes, sensor_columns, colors):
    ax.set_facecolor('#eaeaf2')  # Example: lighter grey background for each subplot
    ax.plot(df_time_filtered.index, df_time_filtered[sensor], label=sensor, color=color)
    ax.set_title(sensor)
    ax.legend(loc='upper right')

# Set the date format on the x-axis
axes[-1].xaxis.set_major_locator(mdates.MinuteLocator(interval=30))  # Set major ticks to every 30 minutes
axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))  # Show only hours and minutes

# Rotate date labels for better readability
plt.setp(axes[-1].get_xticklabels(), rotation=45, ha='right')

# Adjust layout
plt.tight_layout()


# Show the plot
plt.show()







# Assuming 'df' is your DataFrame with DatetimeIndex
# Extract start times of failures
failure_start_times = df[df['Failure Type'] != 'No Failure'].index.unique()

# Define the time window (2 hours)
time_window = pd.Timedelta(hours=2)

# Initialize a new column for the binary label
df['imminent_failure'] = 0

# Iterate through the unique failure start times
for start_time in failure_start_times:
    # Calculate the start of the imminent failure window
    window_start = start_time - time_window

    # Update the imminent_failure column within this window
    df.loc[window_start:start_time, 'imminent_failure'] = 1






import matplotlib.pyplot as plt
import seaborn as sns

# Count plot for imminent failures
sns.countplot(x=df['imminent_failure'])
plt.title('Distribution of Imminent Failures')
plt.xlabel('Imminent Failure')
plt.ylabel('Count')
plt.show()






# Creating a histogram of imminent failures over time
plt.figure(figsize=(10, 6))
df[df['imminent_failure'] == 1].index.to_series().dt.floor('d').value_counts().sort_index().plot(kind='bar')
plt.title('Histogram of Imminent Failures Over Time')
plt.xlabel('Date')
plt.ylabel('Count of Imminent Failures')
plt.xticks(rotation=45)
plt.show()






import matplotlib.pyplot as plt
import pandas as pd

# List of failure windows with some buffer time added
failure_windows = [
    ('2022-02-28 21:53:00', '2022-03-01 02:00:00'),
    ('2022-03-23 14:54:00', '2022-03-23 15:24:00'),
    ('2022-05-30 12:00:00', '2022-06-02 06:18:00')
]

# Convert strings to datetime
failure_windows = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in failure_windows]

# Add buffer time (e.g., 30 minutes before and after)
buffer = pd.Timedelta(minutes=180)
failure_windows = [(start - buffer, end + buffer) for start, end in failure_windows]

# Function to plot each failure window
def plot_failure_window(df, start, end):
    window_df = df[(df.index >= start) & (df.index <= end)]

    plt.figure(figsize=(15, 6))
    plt.plot(window_df.index, window_df['Motor_current'], label='Motor Current')  # Replace 'Motor_current' with your sensor column
    plt.scatter(window_df[window_df['imminent_failure'] == 1].index, window_df[window_df['imminent_failure'] == 1]['Motor_current'], color='red', label='Imminent Failure', marker='x')
    plt.axvspan(start + buffer, end - buffer, color='yellow', alpha=0.3, label='Actual Failure Period')

    plt.title(f'Motor Current from {start} to {end}')
    plt.xlabel('Time')
    plt.ylabel('Motor Current')
    plt.legend()
    plt.show()

# Plot each failure window
for start, end in failure_windows:
    plot_failure_window(df, start, end)



df.columns


print(df['Failure Type'].value_counts())
print(df['Failure Component'].value_counts())


from sklearn.preprocessing import LabelEncoder

# Initialize the encoder
encoder = LabelEncoder()

# Fit and transform 'Failure Component' column
df['Failure Component Encoded'] = encoder.fit_transform(df['Failure Component'])
df['Failure Type Encoded'] = encoder.fit_transform(df['Failure Type'])



print(df['Failure Type'].value_counts())
print(df['Failure Component Encoded'].value_counts())
print(df['Failure Component'].value_counts())
print(df['Failure Component Encoded'].value_counts())


df = df.drop(columns=['Failure Component', 'Failure Type'])
memory_usage = df.memory_usage(deep=True).sum()
print("Memory Usage:", memory_usage, "bytes")





# Specify the directory and filename
directory_path = '/mnt/e/PdM/DataX/MetroPT'
filename = 'Modified_df.csv'

# Save the DataFrame as a CSV file in the specified directory
df.to_csv(f'{directory_path}/{filename}')








from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from xgboost import DMatrix

# Assuming df is your DataFrame and it's sorted by timestamp (df.index)
# Define the failure event timestamps
failure_timestamps = df[df['Failure Type'] != 'No Failure'].index.unique()

# Define time blocks for cross-validation
time_blocks = pd.to_datetime(['2022-01-01', '2022-02-28 21:53:00', '2022-03-23 14:54:00', '2022-05-30 12:00:00', '2022-06-03'])
blocks = [(time_blocks[i], time_blocks[i+1]) for i in range(len(time_blocks)-1)]

# Function to calculate TP, FP, FN
def calculate_overlap_metrics(y_true, y_pred):
    TP = FP = FN = 0
    for i in range(len(y_pred)):
        if y_pred[i] == 1:
            if y_true[i] == 1:
                TP += 1
            else:
                FP += 1
        elif y_pred[i] == 0 and y_true[i] == 1:
            FN += 1
    return TP, FP, FN

# Custom cross-validation
kf = KFold(n_splits=3)  # 3-fold cross-validation

# Initialize metrics
metrics = {
    'TP': 0,
    'FP': 0,
    'FN': 0
}

# Cross-validation loop
for train_index, test_index in kf.split(blocks):
    # Convert block timestamps to actual data slices for train and test
    train_blocks = [blocks[i] for i in train_index]
    test_blocks = [blocks[i] for i in test_index]

    train_data = pd.concat([df[(df.index >= start) & (df.index < end)] for start, end in train_blocks])
    test_data = pd.concat([df[(df.index >= start) & (df.index < end)] for start, end in test_blocks])

    X_train, y_train = train_data[['TP3', 'DV_pressure', 'Oil_temperature', 'LPS']], train_data['imminent_failure']
    X_test, y_test = test_data[['TP3', 'DV_pressure', 'Oil_temperature', 'LPS']], test_data['imminent_failure']


    dtrain = DMatrix(X_train, label=y_train)
    dtest = DMatrix(X_test, label=y_test)


    # Create and train the model using XGBoost with updated GPU support parameters
    model = XGBClassifier(tree_method='hist', eval_metric='logloss', use_label_encoder=False, device='cpu')
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Calculate and update metrics
    TP, FP, FN = calculate_overlap_metrics(y_test.values, y_pred)
    metrics['TP'] += TP
    metrics['FP'] += FP
    metrics['FN'] += FN

# Print overall metrics
print(f"Overall Metrics: TP={metrics['TP']}, FP={metrics['FP']}, FN={metrics['FN']}")




